{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57bf1a41-fdc3-4f90-bdfc-e3f26b5adb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Loading training data...\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\A\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\B\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\C\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\D\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\E\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\F\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\G\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\H\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\I\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\J\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\K\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\L\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\M\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\N\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\O\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\P\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\Q\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\R\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\S\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\T\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\U\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\V\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\W\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\X\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\Y\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\Z\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\del\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\nothing\n",
      "Loading images from: C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\\space\n",
      "ðŸ“¦ Loading testing data...\n",
      "âœ… Loaded 28 test images from flat folder.\n",
      "âœ… Label encoder saved as label_encoder.pkl\n",
      "ðŸ§  Training MobileNetV2 CNN model...\n",
      "Epoch 1/7\n",
      "\u001b[1m318/318\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 172ms/step - accuracy: 0.5748 - loss: 1.5915 - val_accuracy: 0.8214 - val_loss: 0.9675\n",
      "Epoch 2/7\n",
      "\u001b[1m318/318\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 153ms/step - accuracy: 0.9699 - loss: 0.1643 - val_accuracy: 0.9286 - val_loss: 0.8143\n",
      "Epoch 3/7\n",
      "\u001b[1m318/318\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 151ms/step - accuracy: 0.9899 - loss: 0.0689 - val_accuracy: 0.9286 - val_loss: 0.8461\n",
      "Epoch 4/7\n",
      "\u001b[1m318/318\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 157ms/step - accuracy: 0.9968 - loss: 0.0350 - val_accuracy: 0.8929 - val_loss: 0.9760\n",
      "Epoch 5/7\n",
      "\u001b[1m318/318\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 150ms/step - accuracy: 0.9945 - loss: 0.0288 - val_accuracy: 0.9286 - val_loss: 1.0113\n",
      "Epoch 6/7\n",
      "\u001b[1m318/318\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 151ms/step - accuracy: 0.9976 - loss: 0.0181 - val_accuracy: 0.9286 - val_loss: 0.9474\n",
      "Epoch 7/7\n",
      "\u001b[1m318/318\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 154ms/step - accuracy: 0.9970 - loss: 0.0170 - val_accuracy: 0.9286 - val_loss: 1.2186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CNN model saved as asl_mobilenetv2.h5\n",
      "ðŸŒ² Training Random Forest (50 trees)...\n",
      "ðŸš€ Training XGBoost (50 trees)...\n",
      "\n",
      "âœ… Ensemble Accuracy: 0.9285714285714286\n",
      "ðŸ“Š Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       1.00      1.00      1.00         1\n",
      "           3       1.00      1.00      1.00         1\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00         1\n",
      "           6       1.00      1.00      1.00         1\n",
      "           7       1.00      1.00      1.00         1\n",
      "           8       1.00      1.00      1.00         1\n",
      "           9       1.00      1.00      1.00         1\n",
      "          10       1.00      1.00      1.00         1\n",
      "          11       1.00      1.00      1.00         1\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      0.50      0.67         2\n",
      "          14       1.00      1.00      1.00         1\n",
      "          15       1.00      1.00      1.00         1\n",
      "          16       1.00      1.00      1.00         1\n",
      "          17       1.00      1.00      1.00         1\n",
      "          18       1.00      0.50      0.67         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         1\n",
      "          21       1.00      1.00      1.00         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "          23       1.00      1.00      1.00         1\n",
      "          24       1.00      1.00      1.00         1\n",
      "          25       1.00      1.00      1.00         1\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.93        28\n",
      "   macro avg       0.93      0.89      0.90        28\n",
      "weighted avg       1.00      0.93      0.95        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import mode\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# ----------- Load Training Data from Folder Structure -----------\n",
    "def load_data_from_folder(base_path):\n",
    "    X, y = [], []\n",
    "    max_per_class = 350  # Increase for better performance\n",
    "    for label in sorted(os.listdir(base_path)):\n",
    "        label_folder = os.path.join(base_path, label)\n",
    "        if not os.path.isdir(label_folder):\n",
    "            continue\n",
    "        print(f\"Loading images from: {label_folder}\")\n",
    "        img_count = 0\n",
    "        for img_file in os.listdir(label_folder):\n",
    "            if img_count >= max_per_class:\n",
    "                break\n",
    "            img_path = os.path.join(label_folder, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                img = cv2.resize(img, (75, 75))\n",
    "                img = np.stack((img,) * 3, axis=-1)  # Convert to 3 channels\n",
    "                X.append(img)\n",
    "                y.append(label)\n",
    "                img_count += 1\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ----------- Load Flat Test Images from Filenames -----------\n",
    "def load_test_images_from_single_folder(base_path):\n",
    "    X, y = [], []\n",
    "    for filename in sorted(os.listdir(base_path)):\n",
    "        if filename.lower().endswith(('.jpg', '.png')):\n",
    "            label = filename[0].upper()\n",
    "            if label not in list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"):\n",
    "                continue\n",
    "            img_path = os.path.join(base_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                img = cv2.resize(img, (75, 75))\n",
    "                img = np.stack((img,) * 3, axis=-1)\n",
    "                X.append(img)\n",
    "                y.append(label)\n",
    "    print(f\"âœ… Loaded {len(X)} test images from flat folder.\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ----------- Paths to Dataset -----------\n",
    "train_path = r\"C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_train\\asl_alphabet_train\"\n",
    "test_path = r\"C:\\Users\\asusn\\Desktop\\ASL\\asl_alphabet_test\\asl_alphabet_test\"\n",
    "\n",
    "print(\"ðŸ“¦ Loading training data...\")\n",
    "X_train, y_train = load_data_from_folder(train_path)\n",
    "print(\"ðŸ“¦ Loading testing data...\")\n",
    "X_test, y_test = load_test_images_from_single_folder(test_path)\n",
    "\n",
    "if X_test.size == 0 or y_test.size == 0:\n",
    "    print(\"âŒ No test data loaded. Please check test folder.\")\n",
    "    exit()\n",
    "\n",
    "# ----------- Normalize and Encode -----------\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Save the LabelEncoder\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "print(\"âœ… Label encoder saved as label_encoder.pkl\")\n",
    "\n",
    "# Flatten for traditional ML models\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# ----------- CNN using MobileNetV2 -----------\n",
    "def get_mobilenet_model():\n",
    "    base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(75, 75, 3))\n",
    "    base_model.trainable = False  # Freeze for speed\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(len(np.unique(y_train_encoded)), activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"ðŸ§  Training MobileNetV2 CNN model...\")\n",
    "cnn_model = get_mobilenet_model()\n",
    "cnn_model.fit(X_train, y_train_encoded,\n",
    "              epochs=7, validation_data=(X_test, y_test_encoded), verbose=1)\n",
    "\n",
    "# Save the trained CNN model\n",
    "cnn_model.save(\"asl_mobilenetv2.h5\")\n",
    "print(\"âœ… CNN model saved as asl_mobilenetv2.h5\")\n",
    "\n",
    "# Predict with CNN\n",
    "cnn_pred = np.argmax(cnn_model.predict(X_test, verbose=0), axis=1)\n",
    "\n",
    "# ----------- Random Forest -----------\n",
    "print(\"ðŸŒ² Training Random Forest (50 trees)...\")\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf.fit(X_train_flat, y_train_encoded)\n",
    "rf_pred = rf.predict(X_test_flat)\n",
    "\n",
    "# ----------- XGBoost -----------\n",
    "print(\"ðŸš€ Training XGBoost (50 trees)...\")\n",
    "xgb = XGBClassifier(n_estimators=50, eval_metric='mlogloss', use_label_encoder=False)\n",
    "xgb.fit(X_train_flat, y_train_encoded)\n",
    "xgb_pred = xgb.predict(X_test_flat)\n",
    "\n",
    "# ----------- Ensemble Prediction -----------\n",
    "ensemble_preds = np.array([cnn_pred, rf_pred, xgb_pred])\n",
    "final_preds = mode(ensemble_preds, axis=0).mode.flatten()\n",
    "\n",
    "# ----------- Evaluation -----------\n",
    "print(\"\\nâœ… Ensemble Accuracy:\", accuracy_score(y_test_encoded, final_preds))\n",
    "print(\"ðŸ“Š Classification Report:\\n\", classification_report(y_test_encoded, final_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4edb8d9d-9cb5-479a-bd61-bad2adf155ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“· Starting Webcam... Press 'q' to quit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "import mediapipe as mp\n",
    "\n",
    "# Assuming the models and label encoder are already loaded\n",
    "# cnn_model, rf, xgb, and le should be defined before running this code\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“· Starting Webcam... Press 'q' to quit.\\n\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "buffer_text = \"\"\n",
    "last_prediction_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            h, w, _ = frame.shape\n",
    "            x_coords = [lm.x * w for lm in hand_landmarks.landmark]\n",
    "            y_coords = [lm.y * h for lm in hand_landmarks.landmark]\n",
    "            xmin, xmax = int(min(x_coords)), int(max(x_coords))\n",
    "            ymin, ymax = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "            pad = 20\n",
    "            xmin = max(xmin - pad, 0)\n",
    "            ymin = max(ymin - pad, 0)\n",
    "            xmax = min(xmax + pad, w)\n",
    "            ymax = min(ymax + pad, h)\n",
    "\n",
    "            roi = frame[ymin:ymax, xmin:xmax]\n",
    "            if roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Preprocess ROI\n",
    "            img = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (75, 75))\n",
    "            img = np.stack((img,) * 3, axis=-1)  # Convert to 3 channels\n",
    "            img_norm = img.astype('float32') / 255.0\n",
    "\n",
    "            img_cnn = np.expand_dims(img_norm, axis=0)           # For CNN\n",
    "            img_flat = img_norm.reshape(1, -1)                    # For RF/XGB\n",
    "\n",
    "            # Model predictions\n",
    "            pred_cnn = np.argmax(cnn_model.predict(img_cnn, verbose=0))\n",
    "            pred_rf = rf.predict(img_flat)[0]\n",
    "            pred_xgb = xgb.predict(img_flat)[0]\n",
    "\n",
    "            # Majority vote\n",
    "            predictions = np.array([pred_cnn, pred_rf, pred_xgb])\n",
    "            majority_vote = np.atleast_1d(mode(predictions, axis=0).mode)[0]\n",
    "            predicted_letter = le.inverse_transform([majority_vote])[0]\n",
    "\n",
    "            # Update buffer every 1 second\n",
    "            current_time = time.time()\n",
    "            if current_time - last_prediction_time > 1.0:\n",
    "                buffer_text += predicted_letter\n",
    "                last_prediction_time = current_time\n",
    "\n",
    "            # Display prediction box\n",
    "            cv2.rectangle(frame, (xmin, ymin - 40), (xmin + 60, ymin - 10), (0, 128, 255), -1)\n",
    "            cv2.putText(frame, predicted_letter, (xmin + 5, ymin - 15),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "    # Display buffer text\n",
    "    cv2.putText(frame, f'Typed: {buffer_text}', (10, 450),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"ASL Sign Recognition with Hand Tracking\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key & 0xFF == ord('q'):\n",
    "        break\n",
    "    elif key & 0xFF == ord('c'):\n",
    "        buffer_text = \"\"  # Clear buffer\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753c4ee4-7389-4988-ae56-0d7374ac086e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
